---
title: "Final Data Analysis Project"
date:  "See Parts for Write-Up due Dates"
output: pdf_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
if(!require(DataExplorer)){
  install.packages('DataExplorer') 
  library(DataExplorer)
}
if(!require(janitor)){
  install.packages('janitor') 
  library(janitor)
}
library(BAS)
library(car)
library(ggplot2)
library(tidyverse)
library(GGally)
library(knitr)
library(earth)
library(randomForest)
library(caret)
library(vip)
library(scales)
knitr::opts_chunk$set(echo = FALSE,error = TRUE, warning = FALSE, message = FALSE)
```

1. Introduction (1 point if improved from before)

We were hired as statistical consultants by an art historian to explore (1) what drove the prices of 18th century paintings in France, and (2) which paintings may be overvalued or undervalued. They have provided us with auction price data from 1764-1780 on the sales (seller/buyer), painter, and other characteristics of paintings.

In part 1, we perform exploratory data analysis to gain high-level insights into the art data, to help us inform further portions, and run a simple model based on these observations. Our objectives of part 1 include:

1. Subsetting data to only consider variables that are both relevant and not redundant of other explanatory variables. For instance, the data contains the variables `material`, `mat`, `materialCat`. These three variables are all conveying similar information, but expressed differently. Which is most appropriate to include in the model?

2. Transforming key text variables to be standardized and in a form that we can pass into regressions and other predictive modeling algorithms. For instance, if we wanted to include the painter (`authorstandard`) in our model, we first may want to remove stop words in the variable such as ("in the taste of"), and may want to count a collaboration between multiple painters (separated by a semicolon in the data) into each individual painter level, rather than count it into a new level of the variable expressing the collaboration separately from the individual painter levels.

3. Impute values of data when they are missing. For instance, the surface area (`Surface`) does not always exist, so we impute the missing data conditional on variables such as `Shape`, `Height_in`, `Width_in`, and `Diam_in` that may give insight into `Surface`.

4. Examining what sources of variation are associated with variations in price for 18th century art sold in France. For instance, given a specific painter, do prices vary significantly? Is the variation for paintings within-painter variation, or across-painter variation?

5. When controlling for all other variables in the dataset, what which variables are most important to predicting the log of the price of paintings. For instance, what is the marginal impact in driving price if it is an Adrien van de Velde painting, holding all other variables constant?

6. Based on the EDA, run an initial model, and examine the in-sample residuals and coverage.

In part 2, we move beyond just using linear models, and try to evaluate art prices and understand the importance of different variables with more complex methods. Specifically, our added objectives for part 2 include:

7. Examine if we should make any variable transformations, interactions, or further refined text cleaning/grouping for our final dataset in part 1.

8. Implement and evaluate model types aside from linear regression to the data, with the intention of further improving test set RMSE and coverage while still maintaining a lot of the interpretability in part 1 (for the purpose of communicating trends and big pictures ideas to our client).


```{r read-data, echo=FALSE}
load("paintings_train.Rdata")
load("paintings_test.Rdata")
load("paintings_validation.Rdata")
```


2. Exploratory data analysis (1 point if improved from before): 

We once again explain in Parts (a)-(e) what we did for cleaning in part 1 of the assignment. If you read part 1, feel free to skip these parts and instead look at the update.

#### Part (a) Remove redundancies


There are many redundant variables in the data, so for the purposes of illustration, we show below a sample variable selection choice to remove a redundancy. Note that we used a similar process to remove further redundant variables.
```{r winningbiddertype-endbuyer, echo=FALSE}
kable(table(paintings_train$winningbiddertype, paintings_train$endbuyer, useNA = "ifany"), caption = "Winning Bidder Type vs. End Buyer")
```

We see that the last digit of `winningbiddertype` always corresponds to the `endbuyer`. We also see, however, that many of the `winningbiddertype` categories are scarcely populated (`EBC`, for instance, only has 1 value in the training data). Therefore, we use `endbuyer`, and exclude `winningbiddertype` after this step.

The following are the variables we excluded (with the variable that conveys similar information in the parentheses) : `origin_author`, `school_pntg`, `diff_origin` (`origin_cat`); `author` (`author_standard`); `winningbiddertype` (`endbuyer`); `type_intermed` (`Interm`); `Surface_Rect`, `Surface_Rnd`, `Height_in`, `Width_in`, `Diam_in` (`Surface`); `materialCat`, `mat` (`material`); `nfigures` (`singlefig`, `figures`); `landsALL` (`lands_*` variables); `lands_ment` (`lands_sc`, `lands_figs`); `lands_elem` (`lands_sc`, `lands_figs`).

#### Part (b) Cleaning text data

For the purposes of illustration, we only go through the cleaning process for the `authorstandard` variable, though a similar process was applied to `material`, `year`, and `Shape`.

Without any cleaning, it is a good idea to see a set of sample values of the text variable. Below is the 10 most common values for the authors, uncleaned:

```{r uncleaned_authors, echo=FALSE}
uncleaned_authors <- paintings_train %>% 
  group_by(authorstandard) %>% 
  summarize(total=n()) %>% 
  arrange(desc(total)) %>% 
  slice(1:10)
utf8 <- which(Encoding(uncleaned_authors$authorstandard) == "UTF-8")
uncleaned_authors$authorstandard[utf8] <- iconv(uncleaned_authors$authorstandard[utf8], "UTF-8", "UTF-8",sub='') 
kable(uncleaned_authors, caption= "10 most Frequent Authors, Uncleaned")
```

To start, we noticed that some `authorstyle`s were included in the `authorstandard` description, where an `n/a` value corresponds to an original by the said author, and all other values correspond to derivations of the author's original work:

```{r copy_values, echo=FALSE}
kable(table(paintings_train$authorstyle), caption="Types of Derivations of Painters' Work", col.names = c("Author Style", "Frequency"))
```

For the latter group, we marked all such rows as a copy (by creating a variable `is_copy`), and removed these keywords from the author. Additionally, there are some instances where multiple authors contributed to a painting. As a result, we created a binary variable for each other who contributed to at least 10 paintings, where a 1 indicates contribution. Below, observe the sample counts for the most frequent set of authors:

```{r lookup_tables, echo=FALSE}
impute_surface_area <- function(data){
  surface1 <-paintings_train %>%
    mutate(Shape2 = ifelse(Shape %in% c("ronde", "ovale"), 
                           ifelse(Shape == "ronde", "round", "oval"), Shape)) %>%
    group_by(Shape2) %>%
    summarize(Surface = median(Surface, na.rm=TRUE),
              Surface_is_na = 1,
              Height_is_na = 1,
              Width_is_na = 1,
              Diam_is_na = 1) 
  
  surface2 <- paintings_train %>%
      mutate(Shape2 = ifelse(Shape %in% c("ronde", "ovale"), 
                             ifelse(Shape == "ronde", "round", "oval"), Shape),
             Height_in2 = round(Height_in)) %>%
    group_by(Shape2, Height_in2) %>%
    summarize(Surface = median(Surface, na.rm=TRUE),
              Surface_is_na = 1,
              Height_is_na = 0,
              Width_is_na = 1,
              Diam_is_na = 1)
  
  surface3 <- paintings_train %>%
      mutate(Shape2 = ifelse(Shape %in% c("ronde", "ovale"), 
                             ifelse(Shape == "ronde", "round", "oval"), Shape),
             Width_in2 = round(Width_in)) %>%
    group_by(Shape2, Width_in2) %>%
    summarize(Surface = median(Surface, na.rm=TRUE),
              Surface_is_na = 1,
              Height_is_na = 1,
              Width_is_na = 0,
              Diam_is_na = 1)
  
  surface4 <- paintings_train %>%
      mutate(Shape2 = ifelse(Shape %in% c("ronde", "ovale"), 
                             ifelse(Shape == "ronde", "round", "oval"), Shape),
             Diam_in2 = round(Diam_in)) %>%
    group_by(Diam_in2) %>%
    summarize(Surface = median(Surface, na.rm=TRUE),
              Surface_is_na = 1,
              Height_is_na = 1,
              Width_is_na = 1,
              Diam_is_na = 0)
  
  data <- data %>%
    mutate(Shape2 = ifelse(Shape %in% c("ronde", "ovale"), 
                           ifelse(Shape == "ronde", "round", "oval"), Shape),
           Surface_is_na = ifelse(is.na(Surface), 1, 0),
           Height_is_na = ifelse(is.na(Height_in), 1, 0),
           Width_is_na = ifelse(is.na(Width_in), 1, 0),
           Diam_is_na = ifelse(is.na(Diam_in), 1, 0),
           Height_in2 = round(Height_in),
           Width_in2 = round(Width_in),
           Diam_in2 = round(Diam_in)) %>%
    left_join(surface1, by = c("Shape2", "Surface_is_na", "Height_is_na",
                               "Width_is_na", "Diam_is_na"), 
              suffix = c("", "1")) %>%
    left_join(surface2, by = c("Shape2", "Surface_is_na", "Height_is_na",
                               "Width_is_na", "Diam_is_na", "Height_in2"), 
              suffix = c("", "2")) %>%
    left_join(surface3, by = c("Shape2", "Surface_is_na", "Height_is_na",
                               "Width_is_na", "Diam_is_na", "Width_in2"), 
              suffix = c("", "3")) %>%
    left_join(surface4, by = c("Surface_is_na", "Height_is_na",
                               "Width_is_na", "Diam_is_na", "Diam_in2"), 
              suffix = c("", "4")) %>%
    mutate(Surface = coalesce(Surface, Surface1, Surface2, Surface3, Surface4)) %>%
    select(-Shape2, -Surface_is_na, -Height_is_na, -Width_is_na, -Diam_is_na,
           -Height_in2, -Width_in2, -Diam_in2, -Surface1, -Surface2, -Surface3, -Surface4)
  
  return(data)
}
```

```{r cleaning, echo=FALSE}
clean_general <- function(paintings_train){
  index <- sapply(paintings_train,typeof) == "character"
  factor_names <- names(index)[index]
  numeric_names <- names(index)[!index]
  paintings_train[paintings_train == "" | paintings_train == "n/a" | paintings_train == "Unknown" | paintings_train == "NA"] <- NA
  years_df <- paintings_train %>% 
    group_by(year) %>% 
    summarise(avg_logprice=mean(logprice))
  
  # clean year
  for (i in unique(paintings_train$year)){
    paintings_train[,paste0("is_",i)]=ifelse(paintings_train$year==i,1L,0L)
  }
  
  # clean shape
  paintings_train <- paintings_train %>% 
    mutate(squ_rect_oval = ifelse(grepl("squ_rect", Shape)|grepl("oval", Shape), 1L, 0L),
           round = ifelse(grepl("ro", Shape), 1L, 0L))
  
  # clean material
  paintings_train <- paintings_train %>%
    mutate(is_bronze = ifelse(grepl("bronze", material), 1L, 0L),
           is_wood = ifelse(grepl("bois", material), 1L, 0L),
           is_canvas = ifelse(grepl("toile", material), 1L, 0L),
           is_copper = ifelse(grepl("cuivre", material), 1L, 0L),
           is_glass = ifelse(grepl("verre", material), 1L, 0L),
           is_oil = ifelse(grepl("huile", material), 1L, 0L),
           is_paper = ifelse(grepl("papier", material), 1L, 0L),
           is_pastel = ifelse(grepl("pastel", material), 1L, 0L),
           is_cardboard = ifelse(grepl("carton", material), 1L, 0L),
           is_board = ifelse(grepl("tableau", material), 1L, 0L))
}
clean_author <- function(paintings_train){
  # clean author
  paintings_train <- paintings_train %>% 
    mutate(is_copy = ifelse(is.na(paintings_train$authorstyle),0L,1L))
  
  author_style_df <- paintings_train %>%
    mutate(price = exp(logprice)) %>%
    group_by(authorstyle) %>%
    summarize(total_paintings = n(),
              log_mean_price = log(mean(price))) %>%
    arrange(desc(total_paintings))
  
  temp1 <- paintings_train %>%
    mutate(authorstandard2 = 
             trimws(str_replace_all(authorstandard, 
                             paste(author_style_df$authorstyle[author_style_df$authorstyle != "n/a"], collapse="|"), "")))
  
  author_standard2_df <- temp1 %>%
    mutate(price = exp(logprice)) %>%
    group_by(authorstandard2) %>%
    summarize(total_paintings = n(),
              log_mean_price = log(mean(price))) %>%
    arrange(desc(total_paintings))
  
  ind_authors <- author_standard2_df %>%
    separate(authorstandard2, c("Author_1", "Author_2"), sep=";") %>%
    gather(key="Author_num", value="Author", -total_paintings, -log_mean_price) %>%
    select(-Author_num) %>%
    filter(!is.na(Author)) %>%
    mutate(Author = trimws(Author)) %>%
    group_by(Author) %>%
    summarize(log_mean_price = log(sum(total_paintings*exp(log_mean_price))/sum(total_paintings)),
              total_paintings = sum(total_paintings)) %>%
    arrange(desc(total_paintings)) %>%
    filter(total_paintings >= 10)
  
  paintings_train2 <- paintings_train %>%
    mutate(authorstandard2 = 
             trimws(str_replace_all(authorstandard, 
                             paste(author_style_df$authorstyle[author_style_df$authorstyle != "n/a"],
                                   collapse="|"), ""))) %>%
    separate(authorstandard2, c("Author_1", "Author_2"), sep=";") %>%
    mutate(Author_1 = trimws(Author_1),
           Author_2 = trimws(Author_2))
    
  return(list(ind_authors= ind_authors,
              paintings_train2 = paintings_train2))
}
clean <- function(df){
  train <- clean_general(paintings_train)
  df2 <- clean_general(df)
  train2 <- clean_author(train)
  ind_authors = train2$ind_authors
  df2 <- clean_author(df2)$paintings_train2
  
  for(i in as.character(ind_authors$Author)){
    df2[,paste0("is_",i)]=
      ifelse(grepl(i, df2$Author_1, fixed=TRUE) | grepl(i,df2$Author_2, fixed=TRUE),1L,0L)
  }
  
  df2 <- impute_surface_area(df2)
  
  
  interms <- paintings_train %>%
    filter(Interm==1) %>%
    mutate(winningbidder2 = 
             trimws(str_replace_all(winningbidder, " for ", ";")))
  
  interms_df <- interms %>%
    mutate(price = exp(logprice)) %>%
    group_by(winningbidder2) %>%
    summarize(total_paintings = n(),
              log_mean_price = log(mean(price))) %>%
    arrange(desc(total_paintings))
  
  
  ind_endbuyer <- interms_df %>%
    separate(winningbidder2, c("WinningBidder_1", "WinningBidder_2", "WinningBidder_3"), sep=";") %>%
    mutate(EndBuyer = trimws(coalesce(WinningBidder_3, WinningBidder_2, WinningBidder_1)))%>%
    group_by(EndBuyer) %>%
    summarize(log_mean_price = log(sum(total_paintings*exp(log_mean_price))/sum(total_paintings)),
              total_paintings = sum(total_paintings)) %>%
    arrange(desc(total_paintings))
  
  
  winning_bidder_df <- paintings_train %>%
    filter(Interm!=1 | is.na(Interm)) %>%
    mutate(price = exp(logprice),
           winningbidder2 = 
             trimws(str_replace_all(winningbidder, " for ", ";"))) %>%
    group_by(winningbidder2) %>%
    summarize(total_paintings = n(),
              log_mean_price = log(mean(price))) %>%
    arrange(desc(total_paintings))
  
  ind_winningbidders <- winning_bidder_df %>%
    separate(winningbidder2, c("WinningBidder_1", "WinningBidder_2", "WinningBidder_3"), sep=";") %>%
    gather(key="WinningBidder_num", value="WinningBidder", -total_paintings, -log_mean_price) %>%
    select(-WinningBidder_num) %>%
    filter(!is.na(WinningBidder)) %>%
    mutate(EndBuyer = trimws(WinningBidder)) %>%
    group_by(EndBuyer) %>%
    summarize(log_mean_price = log(sum(total_paintings*exp(log_mean_price))/sum(total_paintings)),
              total_paintings = sum(total_paintings)) %>%
    arrange(desc(total_paintings))
  
  #combining both data frames
  buyer_df <- bind_rows(ind_endbuyer, ind_winningbidders) %>%
    group_by(EndBuyer) %>%
    summarize(log_mean_price = log(sum(total_paintings*exp(log_mean_price))/sum(total_paintings)),
              total_paintings = sum(total_paintings)) %>%
    arrange(desc(total_paintings))
  
  buyer_dummy_df <- buyer_df %>%
    filter(total_paintings >= 10) %>%
    mutate(EndBuyer = iconv(EndBuyer, "UTF-8", "",sub=''))
  
  paintings_train3 <- df2 %>%
    mutate(endbuyer1 = trimws(str_replace_all(winningbidder, " for ", ";"))) %>%
    separate(endbuyer1, c("WinningBidder_1", "WinningBidder_2", "WinningBidder_3"), sep=";") %>%
    mutate(EndBuyer = trimws(coalesce(WinningBidder_3, WinningBidder_2, WinningBidder_1))) %>%
    mutate(winningbidder2 = ifelse(Interm==1, EndBuyer, winningbidder),
           winningbidder2 = ifelse(is.na(winningbidder2), "Unknown", winningbidder2),
           winningbidder2 = as.character(iconv(winningbidder2, "UTF-8", "", sub=""))) %>%
    select(-WinningBidder_3, -WinningBidder_2, -WinningBidder_1, -EndBuyer) 
    
  
  for(i in as.character(buyer_dummy_df$EndBuyer)){
    paintings_train3[,paste0("is_",i)]=ifelse(grepl(i, paintings_train3$winningbidder2, fixed=TRUE),1,0)
  }
  
  colMeans(paintings_train3[grepl("is_", colnames(paintings_train3))])*1500
  
  
  #creating keyname categories:
  df2 <- paintings_train3 %>%
    mutate(is_emperor = grepl("[Ee]mpress|[Ee]mperor", winningbidder2),
           is_royal = grepl("[Qq]ueen|[Kk]ing", winningbidder2),
           is_subroyal = grepl("[Pp]rince|[Pp]rincess|[Pp]rincesse", winningbidder2),
           is_duke = grepl("[Dd]uc|[Dd]uchess|[Dd]uchesse", winningbidder2),
           is_count = grepl("[Cc]omte|[Cc]omtesse|[Cc]ount|[Cc]ountess", winningbidder2),
           is_viscount = grepl("[Vv]icomte|[Vv]icomtesse|[Vv]iscount|[Vv]iscountess", winningbidder2),
           is_count = ifelse(is_count+is_viscount>1, 0, is_count),
           is_baron = grepl("[Bb]aron|[Bb]aronne|[Bb]aroness", winningbidder2),
           is_marquis = grepl("[Mm]arquis|[Mm]arquisse|[Mm]arquise", winningbidder2),
           is_lord = grepl("[Ll]ord", winningbidder2))
  
  df2 <- clean_names(df2)
  df2[is.na(df2$interm),"interm"] <- "NA"
  df2[is.na(df2$endbuyer),"endbuyer"] <- "NA"
  df2[is.na(df2$surface)|df2$surface==0,"surface"] <- ifelse(is.na(df2$shape[is.na(df2$surface)|df2$surface==0]),
                                              median(train2$paintings_train2$Surface,na.rm=TRUE),
                                              ifelse(df2$shape[is.na(df2$surface)|df2$surface==0]=="miniature",
                                                     min(train2$paintings_train2$Surface[train2$paintings_train2$Surface > 0], na.rm=TRUE),
                                                     median(train2$paintings_train2$Surface,na.rm=TRUE)))
  index <- sapply(df2,function(x){
    is.integer(x)|is.character(x)
  })
  df2[index] <- lapply(df2[index],factor)
  return(df2)
}
```

```{r cleaned_authors, echo=FALSE}
cleaned_authors <- clean(paintings_train) %>%
  select(is_teniers_ii_the_younger_david:is_potter_pa_paulus) %>%
  gather(key="authorstandard", value="is_author") %>%
  filter(is_author == 1) %>%
  mutate(authorstandard = substring(as.character(authorstandard), 4,
                                    nchar(as.character(authorstandard)))) %>%
  group_by(authorstandard) %>% 
  summarize(total=n()) %>% 
  arrange(desc(total)) %>% 
  slice(1:10)
kable(cleaned_authors, caption= "10 most Frequent Authors, Cleaned")
```

While a lot of the most frequent are the same, we see that some, such as Jan Breughel the Elder, now appear in the data (he contributed with many others in the dataset), and others have an increased count, such as David Teniers the Younger.

#### Part (c) Imputing missing data


It is a good idea to examine which variables contain missing data, and how frequently:
```{r NA plot,fig.height=10, fig.cap="Variables with Missing Values", echo=FALSE}
paintings_train2 <- clean(paintings_train)
temp <- clean_general(paintings_train) %>% 
  select(sale:other)
miss <- sapply(temp,function(x){
  mean(is.na(x))
})
table.miss <- data.frame(cbind(Variables = names(miss) ,`Missing Rate`=round(miss,3))) %>%
  arrange(desc(Missing.Rate)) %>% 
  filter(Missing.Rate!=0) 
  
kable(table.miss, caption = "Missing Variables")
# take out variables by looking the data description and summary
# The following variables are taken out for at least one of the following reasons:
# 1. redundant
# 2. the variable has too many missing values
# 3. not useful
# 4. the variables are broken down into binary variables(year,authorstandard,material,Shape)
train <- paintings_train2 %>% 
  select(-c(sale,lot,price,count,authorstyle,winningbidder,subject,
            other,origin_author,school_pntg,diff_origin,author,
            winningbiddertype,type_intermed,surface_rect,surface_rnd,
            height_in,width_in,diam_in,material_cat,mat,nfigures,
            lands_all,lands_ment,lands_elem,year,authorstandard,material,shape)) %>% 
  clean_names()
```


Since we already removed many of these variables from consideriation in the data redundancy step, we only need to impute values of `Interm`, `endbuyer`, `Surface`. 

Since `Interm` and `endbuyer` are categorical, we created a separate category for the missing data.

Since `Surface` is continuous, we used median imputation. We chose median imputation because the median is less sensitive to outliers than the mean, and we can see that the data is skewed (blue line represents the median, orange line represents the mean):

```{r skewed_surface, echo=FALSE, message=FALSE}
ggplot(data=paintings_train[!is.na(paintings_train$Surface),], aes(x=Surface))+
  geom_histogram()+
  geom_vline(xintercept = median(paintings_train$Surface, na.rm=TRUE), color="#33CCFF")+
  geom_vline(xintercept = mean(paintings_train$Surface, na.rm=TRUE), color="#FF9933")+
  theme_bw()+
  ggtitle("Distribution of Surface Area")+
  theme(plot.title = element_text(hjust=0.5))
  
```

Additionally, we noticed that some covariates were not missing when `Surface` is missing, such as `Shape`, `Height_in`, `Width_in`, and `Diam_in`, so we conditioned our imputed values on the non-missing of these variables. For instance, observation 745 is a square rectangle painting with a height of 17 inches, so our imputed value for the surface area is 340 sq. inches, instead of the imputed 283.5 sq. inches for a square rectangle painting with no height or width filled in.

#### Part (d) Variation of variables

We start by examining the continuous variables:

```{r eda scatter plot, echo=FALSE}
#train <- na.omit(train)
names.quant <- c("logprice","position","surface")
df_quant <- train[names.quant] %>% 
  gather(key = "quant_variable",value = "quant_value",position:surface)
ggplot(df_quant,aes(x= quant_value,y= logprice)) +
  geom_point()+
  facet_wrap(~quant_variable,scales = "free") +
  theme_bw() +
  labs(x = "", title = "logprice vs Quantitative Predictors Scatterplots") +
  stat_smooth(method="lm")+
  theme(plot.title = element_text(hjust=0.5))
```

We see above that position doesn't seem to be strongly related to price, whereas there may be some relationship between surface and price.

Next, we consider categorical variables. Specifically, we want to examine the sources of variation of key discrete variables (within-variable variation or between-variable variation). We look at painters who appear at least 10 times in the dataset, the material of the painting, and the year the painting was sold:

```{r eda boxplot1, echo=FALSE}
selected <- c("dealer","origin_cat","prevcoll","lrgfont","interm","endbuyer",
              "finished","artistliving","year","is_oil","is_paper","is_copy",
              "surface","is_crepin","is_french","is_velde_a_adriaen_van_de",
              "is_wouwerman_philips","is_anonymous")
author_df <-  paintings_train2 %>% 
  select(is_teniers_ii_the_younger_david:is_potter_pa_paulus,logprice)
author_df_long <- author_df %>% 
  gather(key="author", value="is_author", -logprice) %>% 
  mutate(author=substring(author, 4, nchar(author)))%>%
  filter(is_author==1) %>% 
  mutate(selected = ifelse(author %in% c("crepin","french","velde_a_adriaen_van_de",
                                         "wouwerman_philips","anonymous"),"selected","not selected"))
ggplot(data=author_df_long, aes(x=author, y=logprice, fill = factor(selected)))+
  geom_boxplot() +
  theme_bw() +
  labs(x = "Authors", y = "Log Price", fill = "", title = "Log Price vs Authors")+
  theme(axis.text.x = element_text(angle = 90))+
  geom_hline(yintercept = median(paintings_train$logprice))+
  scale_fill_manual(values=c("#33CCFF","#FF9933"))+
  theme(plot.title = element_text(hjust = 0.5))
```

```{r eda boxplot2,fig.height=4, echo=FALSE}
mat_df <- paintings_train2 %>%
  select(is_bronze:is_board,logprice) %>% 
  gather(key = "material",value = "is_material",-logprice) %>% 
  mutate(material=substring(material, 4, nchar(material)))%>%
  filter(is_material==1) %>% 
  mutate(selected = ifelse(material %in% c("oil","paper"),"selected","not selected"))
ggplot(data=mat_df, aes(x=material, y=logprice, fill = factor(selected)))+
  geom_boxplot() +
  theme_bw() +
  labs(x = "Materials", y = "Log Price", fill = "", title = "Log Price vs Materials")+
  geom_hline(yintercept = median(paintings_train$logprice))+
  scale_fill_manual(values=c("#33CCFF","#FF9933"))+
  theme(plot.title = element_text(hjust = 0.5))
```

```{r eda boxplot3,fig.height=4, echo=FALSE}
ggplot(data=paintings_train2, aes(x=factor(year), y=logprice))+
  geom_boxplot(fill="#FF9933") +
  theme_bw() +
  labs(x = "Year", y = "Log Price", title = "Log Price vs Year")+
  geom_hline(yintercept = median(paintings_train$logprice))+
  #scale_fill_manual(values="#FF9933")+
  theme(plot.title = element_text(hjust = 0.5))
```

We can see that there exists both within-class and between-class variation for all three variables, indicating that we may want to include some of the individual levels of the variables (we highlighted the levels that we eventually use in the model in part (3) in orange), but will likely also want to control for other sources of variation.

A final part we wanted to consider in sources of variation is in potential interactions. Specifically, we thought that dealers may specialize in different kinds of art, and therefore have different variation in prices for different kinds of paintings. Below, we consider the dealer's prices conditional on the origin of the work (`origin_cat`):

```{r cond_plots_dealer, echo=FALSE}
ggplot(data=paintings_train2, aes(x=origin_cat, y=logprice, fill = dealer))+
  geom_boxplot() +
  theme_bw() +
  labs(x = "Origin of the Work", y = "Log Price", fill = "Dealer", title = "Log Price vs Origin and Dealer")+
  theme(plot.title = element_text(hjust = 0.5))
```

We can see that dealer "R" is the exclusive dealer in Spanish paintings in this dataset, and they sold for much more than the other pieces he sold. Additionally, it looks like dealer "J" sold a small amount of highly priced works of non-Spanish, non-Dutch, non-French, and non-Italian descent, while the other dealers did not, indicating that perhaps dealer "J" had exclusive access to a specific painter/set of painters outside of western Europe.

#### Part (e) AV-plots

As we saw in part (e), looking at individual boxplots shows some but not all of the variation of key categorical variables. We wanted to further this analysis by robustly considering the marginal impact a variable has holding all other variables constant. Below, we plot add-variable plots of all variables we consider for our model:

```{r avplot1,fig.height=4, echo=FALSE}
formula <- as.formula(paste0("logprice ~ ", paste(paste0(selected), collapse="+")))
par(ask = FALSE)
avPlots(lm(formula, data=paintings_train2), layout = c(3,4),main="")
```

Now, we describe the additional EDA in part 2 of the project:

#### Part (f) Variable transformation examination

Now that we are moving into more complex models, we wanted to look at potential variable transformations or other variables to add to our model.

We noticed specifically that for our model in part 1, paintings with a large surface area also had very high predicted prices. We may have estimated their prices too optimistically if the the trend is in actuality non-linear but we incorrectly used a linear trend. As a result, we wanted to see if we changed the variable scaling if there is any difference. Below is a plot of a log version of `surface` against log price, compared to the original version we used in part 1. 
```{r updated_surface, echo=FALSE}
df_surface <- train %>% 
  select(logprice, surface) %>%
  mutate(log_surface = log(surface)) %>%
  gather(key = "variable",value = "quant_value", surface:log_surface)
ggplot(df_surface,aes(x= quant_value,y= logprice)) +
  geom_point()+
  facet_wrap(~variable,scales = "free") +
  theme_bw() +
  labs(x = "", title = "logprice vs. Surface and log(Surface)") +
  stat_smooth(method="lm")+
  theme(plot.title = element_text(hjust=0.5))
```

We see that this transformation is likely a more appropriate way to use `surface`, as the error region (colored in grey) for the best fit line is more homoscedastic. However, we also see that the slope is smaller in magnitude, indicating that even though the logged version of `surface` may be more appropriate, it may not be important in the model.


#### Part (g) Other considerations

Since all of the other variables in our previous model were discrete variables, the other types of additions to consider in complex models are interactions between different variables. A naive way to go about this is to use a modeling method that can automatically explore interactions (such as trees and GAMs). We do indeed do that, but to gain an intuition why, we wanted to briefly examine a potential interaction.



3. Discussion of preliminary model Part I (5 points)

In part 1, we used a linear regression (chosen through a stepwise-selection process with AIC penalty) to arrive at a model. One thing we did to make our regression model potentially more flexible for out-of-sample data is that we created a lot of discrete variables (such as dummy variables for key authors) and fitted them into our preliminary linear model. The performance of this model had mixed results (note that at the time, the wercker was outputting the wrong result), with a coverage of 95.73% and rmse of 2593.86.  The maximum deviation reaches 47622, which indicates that the difference between our predicted price and actual price is very high for at least 1 observation. 

The large maximum deviation was an opportunity for further EDA, as it is not good for our model to be off by as much as $47,622 from the perspective of the art historian. After investigating our initial model, we found that the variable `surface` was specifically the driver behind the model predicting extremely large values sometimes. As a result, we tried transforming it. However, as we have shown in our updated EDA, this variable does not appear to be very significant after transformation. 

Another thing that we believe might have not helped is the new dummy variables we created for authors. Since each author is observed sparsely, and encoded in a way that can be messy (such as multiple authors working together), we found it hard to properly express all of the information contained in that variable while not introducing a multitude of sparsely populated binary variables. We tried briefly in this portion of the project to use a single `author` variable and express observations with k authors as k separate rows each weighted 1/k (where only the author differs between them), but found that this did not improve on the predictions. As a result, we simplified our model to not include specific information about the artist.

4.  Development of the final model (20 points)

### MARS
We wanted to use a modeling technique that allowed for more flexibility of variables (such as interactions and non-linear relationships) than linear regression, as well as have a method that can still have interpretable results and is easy to compute prediction intervals. As a result, we wanted to work with GAMs, since they are natural extensions to linear regressions (by allowing for knots and polynomial transformations). Specifically, we found that multivariate adaptive regression splines (MARS) to work well for this problem. 

MARS can be seen as a regression extension of CART. It considers all possible binary splits of the categories for a qualitative predictor (or the support for a continuous predictor) and then uses forward selection to chooses the best pairs of piecewise indicator functions until a stopping criterion is met. Leveraging MARS requires minimal specification from the user to execute feature engineering (e.g., feature scaling) and automatically does feature selection, since non-informative features will not be chosen. Furthermore, highly correlated predictors do not impede predictive accuracy as much as they do with OLS models, and the algorithm of MARS allows for easier interpretation and more tractable prediction intervals than tree methods.

#### Final model:

Below is the final model we use:

```{r mars, echo=TRUE}
# mars model
mars <- earth(
  logprice ~ dealer+ diff_origin+ interm+ lrgfont+ origin_author+ 
    origin_cat+ prevcoll+ school_pntg+ endbuyer+ engraved+ figures+
    finished+ history+ lands_ment+ lands_sc+ othgenre+ portrait+ 
    still_life+artistliving,
  data=paintings_train2,
  nfold=10, ncross=30, varmod.method="lm"
)
```

```{r bagged_mars, echo=FALSE}

# mars model
mars2 <- earth(
  logprice ~ dealer+ diff_origin+ interm+ lrgfont+ origin_author+ 
    origin_cat+ prevcoll+ school_pntg+ endbuyer+ engraved+ figures+
    finished+ history+ lands_ment+ lands_sc+ othgenre+ portrait+ 
    still_life+artistliving,
  data=paintings_train2,
  nfold=10, ncross=30, varmod.method="earth"
)
```

#### Variables:

We used a similar set of variables in the model as we considered in part I, with the obvious exceptions of no longer including `surface`, and any `is_*` variable we created. Additionally, we added `diff_origin` and `origin_author`, which we previously excluded from linear regression due to being redundant with `origin_cat` (despite looking significant in the EDA), but can include here since MARS performs selection. `Dealer`, `diff_origin`, `interm`, `lrgfont`, `origin_author`, `origin_cat`, `prevcoll`, `school_pntg`, `endbuyer`, `engraved`, `figures`, `finished`, `history`, `lands_ment`, `lands_sc`, `othgenre`, `portrait`, `still_life` are the 18 variables we considered to fit the MARS model. These variables are the ones seem significant in the EDA part and are chosen from testing.

We show the vaiable selection results Variables Importance Plot below. MARS's variable selection procedure is built upon the GCV (Generalized Cross Validation) $R^2$ value.

```{r var_importance, echo=FALSE}
# variable importance plot
vip(mars, 100) + 
  ggtitle("Variables Importance Plot") +
  theme_bw() +
  theme(plot.title = element_text(hjust=0.5))
# model selection plot
plot(mars, which = 1)
```

We can see specifically that `lrgfont`, `diff_origin`, and when `interm` is NA have a large influence in this model. Additionally, we can observe that MARS did not select some variables entirely, such as `origin_cat`, likely as a result of introducing the redundant `origin_author`.

#### Summary:

Below, we provide a coefficients summary table.
```{r coef_summary, echo=FALSE}
kable(summary(mars)$coefficients, caption="MARS Coefficients Summary")

```


We see that many of the variables that reduced the RSS the most (shown in the variable importance plots) also have larger coefficients here. Additionally, seeing some of these coefficients gives us insight into some levels of variables that were not selected. For instance, the model did not select `endbuyerC` or `endbuyerD`, but did select `endbuyerE` and `endbuyerU`. These two variables have negative coefficients, meaning that it it could be the case that `endbuyer`s who are not in group `E` (expert organizing the sale) or `U` (unknown) may be associated with higher prices of paintings.

#### Residuals:

```{r resid_plot, echo=FALSE}
# residual plot
plot(mars, which = c(3,4))
```

As shown in the residual plots, the residuals are generally centered around zero, are normally distributed, and are homoscedastic, indicating that our model is not obviously biased towards an identifiable subset of the population. 

#### Prediction Intervals:

As for the predictions intervals, we used the interval provided by the earth package when fitting mars model. Specifically, in our initial function call, we used the argument `varmod.method="lm"`, indicating that once `earth()` builds the model, it will then internally apply an `"lm"` (linear regression) to its absolute residuals to estimate the variance conditional on each x. To read more about it in depth, there is a great article here describing the methodology: [http://www.milbo.org/doc/earth-varmod.pdf](http://www.milbo.org/doc/earth-varmod.pdf). In the next section, we show the implications of using a linear model for mapping a variance function versus another MARS model for mapping a variance function.

```{r test_data, echo=FALSE}
paintings_test2 <- clean(paintings_test)
p = predict(mars, newdata=paintings_test2, interval="pint", level=.95)
predictions = as.data.frame(
  exp(p))
save(predictions, file="predict-test.Rdata")
```

```{r train_results, echo=FALSE}
predictions = as.data.frame(
  exp(predict(mars, newdata=paintings_train2, interval="pint", level=.95)))
save(predictions, file="predict-train.Rdata")
```

```{r validation_results, echo=FALSE}
paintings_validation2 <- clean(paintings_validation)
predictions = as.data.frame(
  exp(predict(mars, newdata=paintings_validation2, interval="pint", level=.95)))
save(predictions, file="predict-validation.Rdata")
```

5. Assessment of the final model (25 points)

#### Model evaluation

In order to fit a MARS model, generalized cross-validation is used within the `earth` package, meaning that it is likelier the model will not overfit the training data than a linear regression may. However, it is still a good idea to investigate the in-sample coverage and RMSE to make sure we are within a reasonable range for our model in-sample.

```{r rmse_coverage, echo=FALSE}
predictions.train = as.data.frame(
  exp(predict(mars, newdata=paintings_train2, interval="pint", level=.95)))

predictions.train2 = as.data.frame(
  exp(predict(mars2, newdata=paintings_train2, interval="pint", level=.95)))

coverage_df <- data.frame(actual = paintings_train2$logprice,
                     pred = log(predictions.train$fit),
                     lwr = log(predictions.train$lwr),
                     upr = log(predictions.train$upr),
                     method="Linear Variance Model")

coverage_df2 <- data.frame(actual = paintings_train2$logprice,
                     pred = log(predictions.train2$fit),
                     lwr = log(predictions.train2$lwr),
                     upr = log(predictions.train2$upr),
                     method="MARS Variance Model")

coverage_df3 <- bind_rows(coverage_df, coverage_df2)

ggplot(data= coverage_df3, aes(x = pred, y = actual))+
  geom_ribbon(aes(ymin = lwr, ymax = upr),fill = "blue", alpha = 0.2) +
  geom_point(aes(y=actual))+
  facet_wrap(~method)+
  labs(x = "Predicted Log Price" ,y= "Actual Log Price", title = "95% Prediction Interval for the Training Set")+
  theme_bw()+
  theme(plot.title = element_text(hjust=0.5))
  
rmse = function(ypred, ytest) {
  sqrt(mean((ypred-ytest)^2))
}

mae = function(ypred, ytest) {
  mean(abs(ypred-ytest))
}

kable(cbind(RMSE = paste(round(rmse(predictions.train$fit,exp(paintings_train2$logprice)),2), "livres")),
      caption = "RMSE of the Training Set")
  
```

From the coverage plots of the training set, we can see that the majority of the observations falls between our prediction interval under either set up, which indicates that our method of computing coverage seems appropriate and not too stringent on the functional form. Because we saw the errors of the MARS model are generally homoscedastic, we use the linear model here.

The RMSE of the training set is 1690.57 livres, which was lower than the linear regressions we ran in part I.

### Model testing

```{r test_results, echo=FALSE}
table.test <- cbind(Bias = 254.32,
                    Coverage = 0.948,
                    `Maximum Deviation` = 14558.89,
                    `Mean Absolute Deviation` = 489.12,
                    RMSE = 1305.28)
kable(table.test, caption = "Model Performance for test data")
```

We tested our model using the test data in the wercker. The resulting coverage is about 94.8%. This value is pretty close to 95%, which further indicates that our model is doing good in appropriately accounting for the variance of the data. The test RMSE is about 1305, which is even smaller than the training RMSE, indicating that we are not overfitting our training data. The maximum deviation and mean absolute deviation are 14558.89 and 489.12, respectively. These two values indicate that our estimation is slightly off, and for some observations the difference between the actual price and predicted price is very large. This could happen for the paintings that are significantly overvalued or undervalued. However, it is important to remember that both of these values are smaller in the test set than the training set, so it may be part of variation in the response that we cannot explain with the predictor variables we have right now.

### Model Result

```{r valued_paintings, echo=FALSE}
paintings_validation2 <- clean(paintings_validation)
predictions_val = as.data.frame(
  exp(predict(mars, newdata=paintings_validation2, interval="pint", level=.95)))
paintings_validation3 <- cbind(paintings_validation,predictions_val)
top10 <- paintings_validation3 %>% 
  select(fit,dealer,year, authorstandard, lrgfont) %>% 
  arrange(desc(fit)) %>% 
  slice(1:10)
kable(top10, caption = "Top 10 Valued Paintings in the Validation Set")
```

From the table above, we can see that the top 10 valued paintings are all sold by dealer R and the majority of them are sold in 1769. As we may have expected based on the variable importance plots, having the dealer devote an extra paragraph to a painting is associated with higher value paintings.

It also seems like the paintings painted by Gerrit Dou and Bartholomeus Breenbergh have high values since both of them have two paintings that are valued top 10. Going back to our choice to exclude individual painters, we still are okay with this choice given the limited number of observations of each painter in the data. However, it may be worth in the future trying to set up a model with a Bayesian hierarchical random effects framework if we really wanted to leverage author names, even if they are sparsely populated.


6. Conclusion (10 points): must include a summary of results and a discussion of things learned. Optional what would you do if you had more time.

During the model building procedure, we found that many models (tree models, Ridge and Lasso) don't have tidy built-in methods to calculate prediction intervals, and when using bootstrap to calculate it by ourselves, the results often lead to bad coverage since the bootstrap method can underestimate uncertainty (note that we did not have time to re-submit all of these methods to github after the wercker test data was updated on Thursday night, so the intervals may be better with the updates). 

Additionally, when using Bayesian Model Averaging, we found that while coverage was good, the root mean squared error was fairly high, indicating to us that it was necessary to move beyond linearity.

For us, using MARS, a GAM method that mimicks trees, was a nice compromise between uncovering nonlinearities and kinks in the data with appropriate prediction intervals and digestible interpration. This allowed for good coverage, and great RMSE.

Another key learning moment is that despite our intuition of the author mattering, we could not find a clever way to have it be valuable in our models. Intuitively, famous painters' work is more desired, regardless of the content. However, with little data, it may be hard to assess the level of respect different painters command, especially considering that some may have commanded more respect as they became more known. In a time series dataset like we have here, this is likely too difficult to do.

Looking more directly at the model itself, it seems like the best indicators of a painting's value are the dealer's marketing of it, and the level of respect that that dealer commands. Seeing that so many of the paintings that are predicted to be most valuable in our model come from one dealer when he specifically uses an extra paragraph to advertise it may indicate that people in Paris heavily relied on the word of the dealer to assess the worth of the painting. It would be interesting to see if adjusted for inflation, people today in the art buying business would be as reliant on the dealer's information, or if they could evaluate art more independently. Perhaps it would be worth it to ask the art historian questions like: to the best of your knowledge, did the interaction between buyers, dealers, and painters change from when this data was observed to now? If so, how? Being able to distinguish between paintings that were valuable due to market dynamics at the time and paintings that were valuable because they were actually that good is an important distinction to make, and it is important for us to not assume correlation is causation here.


If we had more time, there are a few things we could consider further:

1. Try doing a bagged MARS model, using the `bagEarth()` function in the `caret` library. This could potentially lead to lower variance predictions, since we can fit multiple MARS models.

2. Change the framework completely to do a Bayesian hierarchical random effects model, incorporating the author, dealer, and potentially even the bidder as random effects. This may stabilize estimates for painters who are sparsely observed in the dataset. 



## Reference 
Mars: <http://uc-r.github.io/mars>
